{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617b0fba-274f-4bdb-802f-69bc33ca2439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL-Skript (Version 15.0 - Finale Syntax) gestartet...\n",
      "-> Datenbankverbindung erfolgreich hergestellt.\n",
      "\n",
      "Lade und bereinige Rohdaten...\n",
      "-> Datenbereinigung abgeschlossen.\n",
      "\n",
      "Schreibe Daten in die Datenbank...\n",
      "-> Alte Daten aus den Tabellen entfernt.\n",
      "-> 31 Kategorien erfolgreich geschrieben.\n",
      "-> 6046 Kanäle erfolgreich geschrieben.\n",
      "\n",
      "Füge Videos zeilenweise ein (das kann einen Moment dauern)...\n",
      "-> 29425 von 29425 Videos erfolgreich geschrieben.\n",
      "\n",
      "Füge tägliche Statistiken für valide Videos ein...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enes\\AppData\\Local\\Temp\\ipykernel_23932\\1899590090.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date_format2 = pd.to_datetime(df_daily_stats['trending_date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 40584 Statistik-Einträge erfolgreich geschrieben.\n",
      "\n",
      "ETL-Prozess erfolgreich abgeschlossen. Alle Daten sind in der Datenbank.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ETL-Skript (Version 15.0 - SQLAlchemy 2.0 Syntax)\n",
    "#\n",
    "# Behebt den finalen Syntaxfehler, indem der TRUNCATE-Befehl explizit\n",
    "# mit der text()-Funktion als SQL-Kommando deklariert wird.\n",
    "#\n",
    "# Autor: [Ihr Name]\n",
    "# Datum: [Heutiges Datum]\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine, text  # NEU: text importiert\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "\n",
    "print(\"ETL-Skript (Version 15.0 - Finale Syntax) gestartet...\")\n",
    "\n",
    "# --- 1. SETUP & DATENBANKVERBINDUNG ---\n",
    "\n",
    "db_url = URL.create(\n",
    "    drivername=\"postgresql\",\n",
    "    username=\"postgres\",\n",
    "    password=\"-\",\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    database=\"youtube_analysis\"\n",
    ")\n",
    "db_engine = create_engine(db_url)\n",
    "\n",
    "with db_engine.connect() as connection:\n",
    "    print(\"-> Datenbankverbindung erfolgreich hergestellt.\")\n",
    "\n",
    "\n",
    "# --- 2. DATEN LADEN UND BEREINIGEN ---\n",
    "\n",
    "print(\"\\nLade und bereinige Rohdaten...\")\n",
    "with open('DE_category_id.json', 'r', encoding='utf-8') as f:\n",
    "    category_json = json.load(f)\n",
    "categories_list = [\n",
    "    {'category_id': int(item['id']), 'category_name': item['snippet']['title']}\n",
    "    for item in category_json['items']\n",
    "]\n",
    "df_categories = pd.DataFrame(categories_list)\n",
    "\n",
    "df_raw = pd.read_csv('DEvideos.csv')\n",
    "\n",
    "text_columns = ['title', 'channel_title', 'tags']\n",
    "for col in text_columns:\n",
    "    df_raw[col] = df_raw[col].str.replace(r'[\\n\\r\\t]', ' ', regex=True).str.replace(\"'\", \"''\")\n",
    "\n",
    "df_raw['channel_title'] = df_raw.groupby('video_id')['channel_title'].transform(lambda x: x.ffill().bfill())\n",
    "df_raw.dropna(subset=['channel_title'], inplace=True)\n",
    "\n",
    "valid_category_ids = df_categories['category_id'].unique()\n",
    "df_raw = df_raw[df_raw['category_id'].isin(valid_category_ids)]\n",
    "\n",
    "print(\"-> Datenbereinigung abgeschlossen.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. DATEN IN DATENBANK SCHREIBEN ---\n",
    "\n",
    "print(\"\\nSchreibe Daten in die Datenbank...\")\n",
    "\n",
    "with db_engine.connect() as connection:\n",
    "    # KORREKTUR: Der SQL-String wird in die text()-Funktion eingewickelt.\n",
    "    truncate_command = text('TRUNCATE TABLE youtube_de.daily_stats, youtube_de.videos, youtube_de.channels, youtube_de.categories RESTART IDENTITY CASCADE;')\n",
    "    connection.execute(truncate_command)\n",
    "    # WICHTIG: SQLAlchemy 2.0 erfordert ein explizites Commit für DDL-Befehle wie TRUNCATE\n",
    "    connection.commit()\n",
    "    print(\"-> Alte Daten aus den Tabellen entfernt.\")\n",
    "\n",
    "df_categories.to_sql('categories', con=db_engine, schema='youtube_de', if_exists='append', index=False)\n",
    "print(f\"-> {len(df_categories)} Kategorien erfolgreich geschrieben.\")\n",
    "\n",
    "df_channels = pd.DataFrame(df_raw['channel_title'].unique(), columns=['channel_title'])\n",
    "df_channels.to_sql('channels', con=db_engine, schema='youtube_de', if_exists='append', index=False)\n",
    "print(f\"-> {len(df_channels)} Kanäle erfolgreich geschrieben.\")\n",
    "\n",
    "print(\"\\nFüge Videos zeilenweise ein (das kann einen Moment dauern)...\")\n",
    "df_channels_from_db = pd.read_sql_table('channels', db_engine, schema='youtube_de')\n",
    "df_videos_unique = df_raw.drop_duplicates(subset=['video_id'], keep='first').copy()\n",
    "df_videos_to_insert = df_videos_unique.merge(df_channels_from_db, on='channel_title', how='left')\n",
    "df_videos_to_insert = df_videos_to_insert[['video_id', 'channel_id', 'title', 'publish_time', 'tags', 'category_id']]\n",
    "\n",
    "successfully_inserted_ids = []\n",
    "for index, row in df_videos_to_insert.iterrows():\n",
    "\n",
    "        single_video_df = pd.DataFrame([row])\n",
    "        single_video_df.to_sql('videos', con=db_engine, schema='youtube_de', if_exists='append', index=False)\n",
    "        successfully_inserted_ids.append(row['video_id'])\n",
    "\n",
    "\n",
    "print(f\"-> {len(successfully_inserted_ids)} von {len(df_videos_to_insert)} Videos erfolgreich geschrieben.\")\n",
    "\n",
    "print(\"\\nFüge tägliche Statistiken für valide Videos ein...\")\n",
    "df_daily_stats = df_raw[df_raw['video_id'].isin(successfully_inserted_ids)].copy()\n",
    "date_format1 = pd.to_datetime(df_daily_stats['trending_date'], format='%y.%d.%m', errors='coerce')\n",
    "date_format2 = pd.to_datetime(df_daily_stats['trending_date'], errors='coerce')\n",
    "combined_dates = date_format1.combine_first(date_format2)\n",
    "df_daily_stats['trending_date'] = combined_dates.dt.strftime('%Y-%m-%d')\n",
    "df_daily_stats_export = df_daily_stats[['video_id', 'trending_date', 'views', 'likes', 'dislikes', 'comment_count', 'comments_disabled', 'ratings_disabled']]\n",
    "\n",
    "df_daily_stats_export.to_sql('daily_stats', con=db_engine, schema='youtube_de', if_exists='append', index=False, method='multi')\n",
    "print(f\"-> {len(df_daily_stats_export)} Statistik-Einträge erfolgreich geschrieben.\")\n",
    "\n",
    "print(\"\\nETL-Prozess erfolgreich abgeschlossen. Alle Daten sind in der Datenbank.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03d199-38d1-426c-81ca-a051ff104fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
